{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9603552,"sourceType":"datasetVersion","datasetId":5859173}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!kaggle datasets download -d crawford/emnist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T03:19:42.467665Z","iopub.execute_input":"2024-10-12T03:19:42.467971Z","iopub.status.idle":"2024-10-12T03:19:52.069805Z","shell.execute_reply.started":"2024-10-12T03:19:42.467937Z","shell.execute_reply":"2024-10-12T03:19:52.068709Z"}},"outputs":[{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/crawford/emnist\nLicense(s): CC0-1.0\nDownloading emnist.zip to /kaggle/working\n 99%|██████████████████████████████████████▋| 1.23G/1.24G [00:07<00:00, 181MB/s]\n100%|███████████████████████████████████████| 1.24G/1.24G [00:07<00:00, 178MB/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision openvino","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T03:19:52.071762Z","iopub.execute_input":"2024-10-12T03:19:52.072098Z","iopub.status.idle":"2024-10-12T03:20:11.286133Z","shell.execute_reply.started":"2024-10-12T03:19:52.072062Z","shell.execute_reply":"2024-10-12T03:20:11.285220Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nCollecting openvino\n  Downloading openvino-2024.4.0-16579-cp310-cp310-manylinux2014_x86_64.whl.metadata (8.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nCollecting openvino-telemetry>=2023.2.1 (from openvino)\n  Downloading openvino_telemetry-2024.1.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from openvino) (21.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->openvino) (3.1.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading openvino-2024.4.0-16579-cp310-cp310-manylinux2014_x86_64.whl (42.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openvino_telemetry-2024.1.0-py3-none-any.whl (23 kB)\nInstalling collected packages: openvino-telemetry, openvino\nSuccessfully installed openvino-2024.4.0 openvino-telemetry-2024.1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install openvino-dev[ONNX]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T03:23:44.038060Z","iopub.execute_input":"2024-10-12T03:23:44.038439Z"}},"outputs":[{"name":"stdout","text":"Collecting openvino-dev[ONNX]\n  Downloading openvino_dev-2024.4.0-16579-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: defusedxml>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (0.7.1)\nCollecting networkx<=3.1.0 (from openvino-dev[ONNX])\n  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (1.26.4)\nRequirement already satisfied: openvino-telemetry>=2023.2.1 in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (2024.1.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (21.3)\nRequirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (6.0.2)\nRequirement already satisfied: requests>=2.25.1 in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (2.32.3)\nRequirement already satisfied: openvino==2024.4.0 in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (2024.4.0)\nCollecting fastjsonschema<2.18,>=2.15.1 (from openvino-dev[ONNX])\n  Downloading fastjsonschema-2.17.1-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: protobuf<4.0.0,>=3.18.1 in /opt/conda/lib/python3.10/site-packages (from openvino-dev[ONNX]) (3.20.3)\nCollecting onnx<=1.16.0,>=1.8.1 (from openvino-dev[ONNX])\n  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.1->openvino-dev[ONNX]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.1->openvino-dev[ONNX]) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.1->openvino-dev[ONNX]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.1->openvino-dev[ONNX]) (2024.8.30)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->openvino-dev[ONNX]) (3.1.2)\nDownloading fastjsonschema-2.17.1-py3-none-any.whl (23 kB)\nDownloading networkx-3.1-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openvino_dev-2024.4.0-16579-py3-none-any.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport time\nimport os\nimport zipfile\nfrom openvino.runtime import Core\nimport numpy as np\n\n# Ensure CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Unzip the EMNIST dataset\nwith zipfile.ZipFile('/kaggle/working/emnist.zip', 'r') as zip_ref:\n    zip_ref.extractall('/kaggle/working/')\n\n# Define the CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 62)  # 62 classes in EMNIST balanced\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return nn.functional.log_softmax(x, dim=1)\n\n# Load and preprocess the EMNIST dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.EMNIST(root='/kaggle/working/emnist', split='balanced', train=True, download=True, transform=transform)\ntest_dataset = datasets.EMNIST(root='/kaggle/working/emnist', split='balanced', train=False, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n\n# Training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = nn.functional.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n\n# Testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += nn.functional.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    \n    test_loss /= len(test_loader.dataset)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n    return accuracy\n\n# Train on GPU\nmodel_gpu = SimpleCNN().to(device)\noptimizer_gpu = optim.Adam(model_gpu.parameters())\n\nstart_time = time.time()\nfor epoch in range(1, 3):  # Train for 2 epochs\n    train(model_gpu, device, train_loader, optimizer_gpu, epoch)\ngpu_train_time = time.time() - start_time\n\ngpu_accuracy = test(model_gpu, device, test_loader)\n\n# Convert to ONNX\ndummy_input = torch.randn(1000,1,28,28).to(device)\ntorch.onnx.export(model_gpu, dummy_input, \"emnist_model.onnx\")\n\n# Optimize with OpenVINO\ncore = Core()\nov_model = core.read_model(\"emnist_model.onnx\")\ncompiled_model = core.compile_model(ov_model, \"CPU\")\n\n# Inference function for OpenVINO model\ndef infer_openvino(model, data):\n    # Reshape the input data to match the expected input shape of the model\n    input_tensor = data.numpy()\n    # Get the expected input shape from the model\n    input_shape = model.input(0).get_shape()  \n    # Reshape the input tensor to match the expected shape, padding with zeros if necessary\n    input_tensor = np.pad(input_tensor, ((0, input_shape[0] - input_tensor.shape[0]), (0, 0), (0, 0), (0, 0)), 'constant')\n    result = model(input_tensor)[0]\n    return torch.from_numpy(result)[:data.shape[0]]  # Only take the relevant part of the output\n\n# Test OpenVINO model\nstart_time = time.time()\ncorrect = 0\nfor data, target in test_loader:\n    output = infer_openvino(compiled_model, data)  # Call the modified infer_openvino function\n    pred = output.argmax(dim=1, keepdim=True)\n    correct += pred.eq(target.view_as(pred)).sum().item()\n\ncpu_inference_time = time.time() - start_time\ncpu_accuracy = 100. * correct / len(test_dataset) # Divide by len(test_dataset) not len(test_loader.dataset)\n\n# GPU Inference Time and FPS Measurement\ngpu_inference_frames = 0\nstart_time = time.time()\nwith torch.no_grad():\n    for data, target in test_loader:\n        data = data.to(device)\n        output = model_gpu(data)\n        gpu_inference_frames += data.size(0)  # Increment frames processed by batch size\ngpu_inference_time = time.time() - start_time\ngpu_inference_fps = gpu_inference_frames / gpu_inference_time if gpu_inference_time > 0 else 0\n\n# CPU (OpenVINO) Inference FPS Measurement\ncpu_inference_frames = 0\nstart_time = time.time()\nfor data, target in test_loader:\n    output = infer_openvino(compiled_model, data)  # Call the modified infer_openvino function\n    cpu_inference_frames += data.size(0)  # Increment frames processed by batch size\ncpu_inference_time = time.time() - start_time\ncpu_inference_fps = cpu_inference_frames / cpu_inference_time if cpu_inference_time > 0 else 0\n\n\n# Print the results\nprint(f\"GPU Training Time: {gpu_train_time:.2f} seconds\")\nprint(f\"GPU Inference Accuracy: {gpu_accuracy:.2f}%\")\nprint(f\"GPU Inference Time: {gpu_inference_time:.2f} seconds\")\nprint(f\"GPU Inference FPS: {gpu_inference_fps:.2f}\")\nprint(f\"CPU (OpenVINO) Inference Time: {cpu_inference_time:.2f} seconds\")\nprint(f\"CPU (OpenVINO) Inference Accuracy: {cpu_accuracy:.2f}%\")\nprint(f\"CPU (OpenVINO) Inference FPS: {cpu_inference_fps:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T03:50:11.691657Z","iopub.execute_input":"2024-10-12T03:50:11.692007Z","iopub.status.idle":"2024-10-12T03:52:11.306229Z","shell.execute_reply.started":"2024-10-12T03:50:11.691973Z","shell.execute_reply":"2024-10-12T03:52:11.305252Z"}},"outputs":[{"name":"stdout","text":"Train Epoch: 1 [0/112800 (0%)]\tLoss: 4.135045\nTrain Epoch: 1 [6400/112800 (6%)]\tLoss: 2.076803\nTrain Epoch: 1 [12800/112800 (11%)]\tLoss: 1.492812\nTrain Epoch: 1 [19200/112800 (17%)]\tLoss: 1.390569\nTrain Epoch: 1 [25600/112800 (23%)]\tLoss: 1.153956\nTrain Epoch: 1 [32000/112800 (28%)]\tLoss: 1.018777\nTrain Epoch: 1 [38400/112800 (34%)]\tLoss: 0.965032\nTrain Epoch: 1 [44800/112800 (40%)]\tLoss: 1.073152\nTrain Epoch: 1 [51200/112800 (45%)]\tLoss: 0.812791\nTrain Epoch: 1 [57600/112800 (51%)]\tLoss: 1.043101\nTrain Epoch: 1 [64000/112800 (57%)]\tLoss: 0.961771\nTrain Epoch: 1 [70400/112800 (62%)]\tLoss: 0.803099\nTrain Epoch: 1 [76800/112800 (68%)]\tLoss: 0.749528\nTrain Epoch: 1 [83200/112800 (74%)]\tLoss: 0.853523\nTrain Epoch: 1 [89600/112800 (79%)]\tLoss: 0.990795\nTrain Epoch: 1 [96000/112800 (85%)]\tLoss: 0.748865\nTrain Epoch: 1 [102400/112800 (91%)]\tLoss: 0.733336\nTrain Epoch: 1 [108800/112800 (96%)]\tLoss: 0.835138\nTrain Epoch: 2 [0/112800 (0%)]\tLoss: 0.540039\nTrain Epoch: 2 [6400/112800 (6%)]\tLoss: 0.678776\nTrain Epoch: 2 [12800/112800 (11%)]\tLoss: 0.701689\nTrain Epoch: 2 [19200/112800 (17%)]\tLoss: 0.882537\nTrain Epoch: 2 [25600/112800 (23%)]\tLoss: 0.854763\nTrain Epoch: 2 [32000/112800 (28%)]\tLoss: 0.969406\nTrain Epoch: 2 [38400/112800 (34%)]\tLoss: 0.537855\nTrain Epoch: 2 [44800/112800 (40%)]\tLoss: 0.544092\nTrain Epoch: 2 [51200/112800 (45%)]\tLoss: 0.526226\nTrain Epoch: 2 [57600/112800 (51%)]\tLoss: 0.693142\nTrain Epoch: 2 [64000/112800 (57%)]\tLoss: 0.780423\nTrain Epoch: 2 [70400/112800 (62%)]\tLoss: 0.538610\nTrain Epoch: 2 [76800/112800 (68%)]\tLoss: 0.514919\nTrain Epoch: 2 [83200/112800 (74%)]\tLoss: 0.712144\nTrain Epoch: 2 [89600/112800 (79%)]\tLoss: 0.775128\nTrain Epoch: 2 [96000/112800 (85%)]\tLoss: 0.564137\nTrain Epoch: 2 [102400/112800 (91%)]\tLoss: 0.718672\nTrain Epoch: 2 [108800/112800 (96%)]\tLoss: 0.897665\n\nTest set: Average loss: 0.4387, Accuracy: 16049/18800 (85.37%)\n\nGPU Training Time: 52.58 seconds\nGPU Inference Accuracy: 85.37%\nGPU Inference Time: 3.78 seconds\nGPU Inference FPS: 4967.58\nCPU (OpenVINO) Inference Time: 5.88 seconds\nCPU (OpenVINO) Inference Accuracy: 85.37%\nCPU (OpenVINO) Inference FPS: 3195.59\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}